{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient 3D Deep Learning with torchsparse and SPVNAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how to efficiently process LiDAR point clouds with our efficient 3D sparse computation library, `torchsparse` and our newly proprosed 3D AutoML framework, SPVNAS.\n",
    "<img src=\"https://hanlab.mit.edu/projects/spvnas/figures/overview.png\" width=\"720\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clone the codebase first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mit-han-lab/e3d.git\n",
    "import os\n",
    "os.chdir('e3d/tutorial')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install necessary libraries used in this tutorial. Note: `pip install` can be time consuming. It takes ~5 minutes on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google's sparse hash project. Used in torchsparse.\n",
    "!sudo apt-get install libsparsehash-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The library used for plotting.\n",
    "!pip install plotly --upgrade 1>/dev/null\n",
    "# torchsparse is our high-performance 3D sparse convolution library.\n",
    "!pip install --upgrade git+https://github.com/mit-han-lab/torchsparse.git 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# torchsparse is our high-performance 3D sparse convolution library.\n",
    "import torchsparse\n",
    "import torchsparse.nn as spnn\n",
    "from torchsparse import SparseTensor\n",
    "from torchsparse.utils import sparse_quantize, sparse_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient 3D Sparse Computation with torchsparse\n",
    "\n",
    "We start the first part of this tutorial, where we will present how to use our library `torchsparse` to load input point cloud data, define networks and do training. The library `torchsparse` is a high-performance computing library for efficient 3D sparse convolution. This library aims at accelerating sparse computation in 3D, in particular the Sparse Convolution operation. \n",
    "\n",
    "<img src=\"https://hanlab.mit.edu/projects/spvnas/figures/sparseconv_illustration.gif\" width=\"720\">\n",
    "\n",
    "The major advantage of this library is that we support all computation on the GPU, especially the kernel map construction (which is done on the CPU in latest [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) V0.4.3). In addition, we support more general 3D modules such as [Sparse Point-Voxel Convolution](https://arxiv.org/abs/2007.16100) presented in our SPVNAS project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce how to process input point cloud with the `torchsparse` library. The core idea is that we want to downsample the input into sparse volumetric representation through the function `sparse_quantize` (which we imported from `torchsparse.utils` just now). In order to perform batching, we convert the format of each input to `torchsparse.SparseTensor`, which is composed of `features (F)` and `coordinates (C)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_point_cloud(input_point_cloud, input_labels, \n",
    "                        voxel_size=0.05, ignore_label=19):\n",
    "    input_point_cloud[:, 3] = input_point_cloud[:, 3]\n",
    "    # get rounded coordinates\n",
    "    pc_ = np.round(input_point_cloud[:, :3] / voxel_size)\n",
    "    pc_ -= pc_.min(0, keepdims=1)\n",
    "    labels_ = input_labels\n",
    "    feat_ = input_point_cloud\n",
    "    # filter out unlabeled points\n",
    "    out_pc = input_point_cloud[labels_ != ignore_label, :3]\n",
    "    pc_ = pc_[labels_ != ignore_label]\n",
    "    feat_ = feat_[labels_ != ignore_label]\n",
    "    labels_ = labels_[labels_ != ignore_label]\n",
    "        \n",
    "    # sparse quantization: filter out duplicate points after downsampling\n",
    "    inds, labels, inverse_map = sparse_quantize(pc_,\n",
    "                                                feat_,\n",
    "                                                labels_,\n",
    "                                                return_index=True,\n",
    "                                                return_invs=True)\n",
    "    # construct members as sparse tensor so that they can be collated\n",
    "    pc = pc_[inds]\n",
    "    feat = feat_[inds]\n",
    "    labels = SparseTensor(\n",
    "        labels_[inds], pc\n",
    "    )\n",
    "    lidar = SparseTensor(\n",
    "        feat, pc\n",
    "    )\n",
    "    targets_mapped = SparseTensor(\n",
    "        labels_, out_pc\n",
    "    )\n",
    "    inverse_map = SparseTensor(\n",
    "        inverse_map, out_pc\n",
    "    )\n",
    "    out_pc = SparseTensor(\n",
    "        out_pc, out_pc\n",
    "    )\n",
    "    # construct the feed_dict\n",
    "    feed_dict = {\n",
    "        'pc': out_pc,\n",
    "        'lidar': lidar,\n",
    "        'targets': labels,\n",
    "        'targets_mapped': targets_mapped,\n",
    "        'inverse_map': inverse_map\n",
    "    }\n",
    "    \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then introduce how to perform batching in `torchsparse`. Here, we assume we receive a list of `feed_dict` from the input preprocessor. What we need to do is to directly invoke the `sparse_collate_fn` function (imported from `torchsparse.utils`) and our library will help you deal with batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_batch(batch_size=2, pc_size=100000, num_classes=10):\n",
    "    lis = []\n",
    "    for i in range(batch_size):\n",
    "        dummy_pc = np.random.randn(pc_size, 4) * 10\n",
    "        dummy_label = np.random.choice(num_classes, pc_size)\n",
    "        feed_dict = process_point_cloud(\n",
    "            dummy_pc,\n",
    "            dummy_label\n",
    "        )\n",
    "        lis.append(feed_dict)\n",
    "    return sparse_collate_fn(lis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you are familiar with input processing pipeline with `torchsparse`. For further reading, you are welcomed to checkout our [SPVNAS codebase](https://github.com/mit-han-lab/e3d/blob/master/spvnas/core/datasets/semantic_kitti.py) for real-world dataset processing. \n",
    "\n",
    "We'll continue to introduce how we can define models with `torchsparse`. Notice that this part is very similar to `PyTorch`, where we have `torchsparse.nn (spnn)` corresponds to `torch.nn (nn)` and `torchsparse.nn.functional (spf)` corresponds to `torch.nn.functional (F)` by convention. The module `spnn.Conv3d` means Sparse 3D Convolution and you can define its input, output channels, kernel size and stride as is shown below. It is also possible to specify whether it is a transposed convolution (*i.e.* one used to upsample the input feature map). Non-spatial operations such as `spnn.BatchNorm` and `spnn.ReLU` are similar to batchnorm / ReLU operations in 2D CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the device to run inference / training\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('We will run training on', device)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "dummy_model = nn.Sequential(\n",
    "    spnn.Conv3d(4, 32, kernel_size=3, stride=1),\n",
    "    spnn.BatchNorm(32),\n",
    "    spnn.ReLU(True),\n",
    "        \n",
    "    spnn.Conv3d(32, 64, kernel_size=2, stride=2),\n",
    "    spnn.BatchNorm(64),\n",
    "    spnn.ReLU(True),\n",
    "        \n",
    "    spnn.Conv3d(64, 64, kernel_size=2, stride=2, transpose=True),\n",
    "    spnn.BatchNorm(64),\n",
    "    spnn.ReLU(True),\n",
    "        \n",
    "    spnn.Conv3d(64, 32, kernel_size=3, stride=1),\n",
    "    spnn.BatchNorm(32),\n",
    "    spnn.ReLU(True),\n",
    "        \n",
    "    spnn.Conv3d(32, num_classes, kernel_size=1)\n",
    ").to(device)\n",
    "\n",
    "print(dummy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform dummy training. The code structure is very similar to conventional 2D CNN training. However, one has to pay special attention to the outputs from the model. In the `dummy_model` defined above, the output is `SparseTensor`, we therefore need to convert it to `torch.Tensor` via `outputs.F` before feeding it to the `criterion`. When the output is `torch.Tensor`, such conversion will be unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dummy_train(model, device, num_classes=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    print('Starting dummy training...')\n",
    "    for i in range(10):\n",
    "        feed_dict = generate_random_batch(\n",
    "            batch_size = 2,\n",
    "            pc_size = 50000\n",
    "        )\n",
    "        inputs = feed_dict['lidar'].to(device)\n",
    "        targets = feed_dict['targets'].F.to(device).long()\n",
    "        outputs = model(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.F, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('[step %d] loss = %f.'%(i, loss.item()))\n",
    "    print('Finished dummy training!')\n",
    "\n",
    "    \n",
    "dummy_train(dummy_model, device, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast and Accurate 3D Deep Learning with SPVNAS\n",
    "\n",
    "Congratulations! You have got yourself familiar with our efficient 3D sparse computing library, `torchsparse`! Now, we will move on to the second part of this tutorial, which is related to our newly proposed SPVNAS at ECCV 2020.\n",
    "\n",
    "[SPVNAS](https://arxiv.org/abs/2007.16100) is the **first** AutoML method for efficient 3D scene understanding. In this work, we first adapt [Point-Voxel Convolution](https://arxiv.org/abs/1907.03739) (NeurIPS 2019) to large-scale outdoor LiDAR scans by introducing Sparse Point-Voxel Convolution (SPVConv):\n",
    "\n",
    "<img src=\"https://hanlab.mit.edu/projects/spvnas/figures/spvconv.png\" width=\"720\">\n",
    "\n",
    "We then apply 3D Neural Architecture Search (3D-NAS) to automatically search for the best architectures built from SPVConv under efficiency constraints.\n",
    "\n",
    "<img src=\"https://hanlab.mit.edu/projects/spvnas/figures/3dnas.png\" width=\"720\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helpers for visualization (`color_map` and `label_map`) and select the device to run inference on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_label_map\n",
    "# color map for visualization\n",
    "color_map = np.array(['#f59664', '#f5e664', '#963c1e', \n",
    "             '#b41e50', '#ff0000', '#1e1eff', \n",
    "             '#c828ff', '#5a1e96', '#ff00ff', \n",
    "             '#ff96ff', '#4b004b', '#4b00af', \n",
    "             '#00c8ff', '#3278ff', '#00af00', \n",
    "             '#003c87', '#50f096', '#96f0ff', \n",
    "             '#0000ff', '#ffffff'])\n",
    "\n",
    "# label map maps SemanticKITTI labels to [0,19]\n",
    "label_map = create_label_map()\n",
    "# define the device to run inference / training\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('We will run inference on', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some real data and process it with aforementioned `process_point_cloud` and `sparse_collate_fn` functions for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prepared data\n",
    "point_cloud = np.fromfile('./sample_data/000000.bin', dtype=np.float32).reshape(-1,4)\n",
    "label = np.fromfile('./sample_data/000000.label', dtype=np.int32) & 0xFFFF\n",
    "label = label_map[label]\n",
    "# use sparse_collate_fn to create batch\n",
    "feed_dict = sparse_collate_fn([process_point_cloud(point_cloud, label)])\n",
    "print('Created feed dict with keys:', feed_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the pretrained SPVNAS model from our model zoo to run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SPVNAS model from model zoo\n",
    "from model_zoo import spvnas_specialized\n",
    "model = spvnas_specialized('SemanticKITTI_val_SPVNAS@65GMACs').to(device)\n",
    "model.eval()\n",
    "# run inference\n",
    "inputs = feed_dict['lidar'].to(device)\n",
    "outputs = model(inputs)\n",
    "predictions = outputs.argmax(1).cpu().numpy()\n",
    "# map predictions from downsampled sparse voxels to original points\n",
    "predictions = predictions[feed_dict['inverse_map'].F.int().cpu().numpy()]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will visualize the predictions from SPVNAS in an interactive window. Please run the following cell and enjoy it! (Notice that rendering the interactive window may take some time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def configure_plotly_browser_state():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "# Import dependencies\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "# Configure Plotly to be rendered inline in the notebook.te\n",
    "\n",
    "pc = feed_dict['pc'].F.cpu().numpy()\n",
    "# Configure the trace.\n",
    "trace = go.Scatter3d(\n",
    "    x=pc[:, 0],\n",
    "    y=pc[:, 1],\n",
    "    z=pc[:, 2],\n",
    "    mode='markers',\n",
    "    marker={\n",
    "        'size': 1,\n",
    "        'opacity': 0.8,\n",
    "        'color': color_map[predictions].tolist(),\n",
    "    }\n",
    ")\n",
    "configure_plotly_browser_state()\n",
    "plotly.offline.init_notebook_mode(connected=False)\n",
    "# Configure the layout.\n",
    "layout = go.Layout(\n",
    "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "    scene=dict(aspectmode=\"manual\", aspectratio=dict(x=1, y=1, z=0.2))\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "plot_figure = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Render the plot.\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've finished the entire notebook on **Efficient 3D Deep Learning with torchsparse and SPVNAS**!\n",
    "\n",
    "If you want to learn more, here are some papers and repos for your reference:\n",
    "\n",
    "[1] Z. Liu, H. Tang, Y. Lin and S. Han. Point-Voxel CNN for Efficient 3D Deep Learning. In NeurIPS 2019, spotlight. [[paper]](https://arxiv.org/abs/1907.03739)\n",
    "\n",
    "[2] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang and S. Han. Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution. In ECCV 2020. [[paper]](https://arxiv.org/abs/2007.16100)\n",
    "\n",
    "[3] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang and S. Han. [GitHub Repo](https://github.com/mit-han-lab/e3d) of Efficient 3D Deep Learning Methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
